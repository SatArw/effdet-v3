{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Use this Jupyter Notebook as a guide to run your trained model in inference mode\n","\n","created by Anton Morgunov\n","\n","inspired by [tensorflow object detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#exporting-a-trained-model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Your first step is going to specify which unit you are going to work with for inference. Select between GPU or CPU and follow the below instructions for implementation."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import os # importing OS in order to make GPU visible\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # do not change anything in here\n","\n","# specify which device you want to work on.\n","# Use \"-1\" to work on a CPU. Default value \"0\" stands for the 1st GPU that will be used\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # TODO: specify your computational device"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-06-02 13:36:30.875605: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-06-02 13:36:30.906739: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-06-02 13:36:30.907202: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-02 13:36:31.671755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["No GPU found\n"]}],"source":["import tensorflow as tf # import tensorflow\n","\n","# checking that GPU is found\n","if tf.test.gpu_device_name():\n","    print('GPU found')\n","else:\n","    print(\"No GPU found\")"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["# other import\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next you will import import scripts that were already provided by Tensorflow API. **Make sure that Tensorflow is your current working directory.**"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/satarw/efficientdet\n","/home/satarw/efficientdet/workspace\n"]}],"source":["import sys # importyng sys in order to access scripts located in a different folder\n","\n","%cd ..\n","path2scripts = './models/research' # TODO: provide pass to the research folder\n","%cd workspace\n","sys.path.insert(0, path2scripts) # making scripts in models/research available for import"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/satarw/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["# importing all scripts that will be needed to export your model and use it for inference\n","from object_detection.utils import label_map_util\n","from object_detection.utils import config_util\n","from object_detection.utils import visualization_utils as viz_utils\n","from object_detection.builders import model_builder"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now you can import and build your trained model:"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# NOTE: your current working directory should be Tensorflow.\n","\n","# TODO: specify two pathes: to the pipeline.config file and to the folder with trained model.\n","path2config ='./exported_models/d0/v3/pipeline.config'\n","\n","path2model = './exported_models/d0/v3/checkpoint/'    "]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["# do not change anything in this cell\n","configs = config_util.get_configs_from_pipeline_file(path2config) # importing config\n","model_config = configs['model'] # recreating model config\n","detection_model = model_builder.build(model_config=model_config, is_training=False) # importing model"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f1203097d60>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n","ckpt.restore(os.path.join(path2model, 'ckpt-0')).expect_partial()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, path to label map should be provided. Category index will be created based on label map file"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["path2label_map = './data/button_label_map.pbtxt' # TODO: provide a path to the label map file\n","category_index = label_map_util.create_category_index_from_labelmap(path2label_map,use_display_name=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, a few supporting functions will be defined"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["def detect_fn(image):\n","    \"\"\"\n","    Detect objects in image.\n","    \n","    Args:\n","      image: (tf.tensor): 4D input image\n","      \n","    Returs:\n","      detections (dict): predictions that model made\n","    \"\"\"\n","\n","    image, shapes = detection_model.preprocess(image)\n","    print(image,shapes)\n","    prediction_dict = detection_model.predict(image, shapes)\n","    detections = detection_model.postprocess(prediction_dict, shapes)\n","\n","    return detections\n","  \n","def load_image_into_numpy_array(path):\n","  \"\"\"Load an image from file into a numpy array.\n","\n","  Puts image into numpy array to feed into tensorflow graph.\n","  Note that by convention we put it into a numpy array with shape\n","  (height, width, channels), where channels=3 for RGB.\n","\n","  Args:\n","    path: the file path to the image\n","\n","  Returns:\n","    numpy array with shape (img_height, img_width, 3)\n","  \"\"\"\n","  \n","  return np.array(Image.open(path))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Next function is the one that you can use to run inference and plot results an an input image:**"]},{"cell_type":"code","execution_count":21,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["def inference_with_plot(path2images, box_th=0.6):\n","    \"\"\"\n","    Function that performs inference and plots resulting b-boxes\n","    \n","    Args:\n","      path2images: an array with pathes to images\n","      box_th: (float) value that defines threshold for model prediction.\n","      \n","    Returns:\n","      None\n","    \"\"\"\n","    i = 0\n","    for image_path in path2images:\n","        i += 1\n","        print('Running inference for {}... '.format(image_path), end='')\n","\n","        image_np = load_image_into_numpy_array(image_path)\n","        \n","        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","        detections = detect_fn(input_tensor)\n","\n","        # All outputs are batches tensors.\n","        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n","        # We're only interested in the first num_detections.\n","        num_detections = int(detections.pop('num_detections'))\n","        detections = {key: value[0, :num_detections].numpy()\n","                      for key, value in detections.items()}\n","      \n","        detections['num_detections'] = num_detections\n","\n","        # detection_classes should be ints.\n","        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","        print(detections)\n","        label_id_offset = 1\n","        image_np_with_detections = image_np.copy()\n","\n","        viz_utils.visualize_boxes_and_labels_on_image_array(\n","                image_np_with_detections,\n","                detections['detection_boxes'],\n","                detections['detection_classes']+label_id_offset,\n","                detections['detection_scores'],\n","                category_index,\n","                use_normalized_coordinates=True,\n","                max_boxes_to_draw=200,\n","                min_score_thresh=box_th,\n","                agnostic_mode=False,\n","                line_thickness=5)\n","\n","        plt.figure(figsize=(15,10))\n","        plt.imshow(image_np_with_detections)\n","        print('Done')\n","        plt.savefig(f\"results{i}.png\")\n","    # plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, we will define a few other supporting functions:"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["def nms(rects, thd=0.5):\n","    \"\"\"\n","    Filter rectangles\n","    rects is array of oblects ([x1,y1,x2,y2], confidence, class)\n","    thd - intersection threshold (intersection divides min square of rectange)\n","    \"\"\"\n","    out = []\n","\n","    remove = [False] * len(rects)\n","\n","    for i in range(0, len(rects) - 1):\n","        if remove[i]:\n","            continue\n","        inter = [0.0] * len(rects)\n","        for j in range(i, len(rects)):\n","            if remove[j]:\n","                continue\n","            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n","\n","        max_prob = 0.0\n","        max_idx = 0\n","        for k in range(i, len(rects)):\n","            if inter[k] >= thd:\n","                if rects[k][1] > max_prob:\n","                    max_prob = rects[k][1]\n","                    max_idx = k\n","\n","        for k in range(i, len(rects)):\n","            if (inter[k] >= thd) & (k != max_idx):\n","                remove[k] = True\n","\n","    for k in range(0, len(rects)):\n","        if not remove[k]:\n","            out.append(rects[k])\n","\n","    boxes = [box[0] for box in out]\n","    scores = [score[1] for score in out]\n","    classes = [cls[2] for cls in out]\n","    return boxes, scores, classes\n","\n","\n","def intersection(rect1, rect2):\n","    \"\"\"\n","    Calculates square of intersection of two rectangles\n","    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n","    return: square of intersection\n","    \"\"\"\n","    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n","    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n","    overlapArea = x_overlap * y_overlap;\n","    return overlapArea\n","\n","\n","def square(rect):\n","    \"\"\"\n","    Calculates square of rectangle\n","    \"\"\"\n","    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Next function is the one that you can use to run inference and save results into a file:**"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","\n","def inference_as_raw_output(path2images,box_th = 0.5,nms_th = 0.5,to_file = False,data = None,path2dir = False):\n","    # print(\"Start\")\n","     \n","    \"\"\"\n","    Function that performs inference and return filtered predictions and prediction times\n","    \n","    Args:\n","      path2images: an array with pathes to images\n","      box_th: (float) value that defines threshold for model prediction. Consider 0.25 as a value.\n","      nms_th: (float) value that defines threshold for non-maximum suppression. Consider 0.5 as a value.\n","      to_file: (boolean). When passed as True => results are saved into a file. Writing format is\n","      path2image + (x1abs, y1abs, x2abs, y2abs, score, conf) for box in boxes\n","      data: (str) name of the dataset you passed in (e.g. test/validation)\n","      path2dir: (str). Should be passed if path2images has only basenames. If full pathes provided => set False.\n","      \n","    Returs:\n","      detections (dict): filtered predictions that model made\n","    \"\"\"\n","     \n","    \n","    print (f'Current data set is {data}')  \n","    print (f'Ready to start inference on {len(path2images)} images!')\n","    times = []\n","    for image_path in tqdm(path2images):\n","        if path2dir: # if a path to a directory where images are stored was passed in\n","            image_path = os.path.join(path2dir, image_path.strip())\n","            \n","        image_np = load_image_into_numpy_array(image_path)\n","\n","        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","        t0 = cv2.getTickCount()\n","        detections = detect_fn(input_tensor)\n","        t1 = cv2.getTickCount()\n","        time = (t1-t0)/cv2.getTickFrequency()\n","        times.append(time)\n","        # print(\"time noted\")\n","        # checking how many detections we got\n","        num_detections = int(detections.pop('num_detections'))\n","        \n","        # filtering out detection in order to get only the one that are indeed detections\n","        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n","        \n","        # detection_classes should be ints.\n","        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","        \n","        # defining what we need from the resulting detection dict that we got from model output\n","        key_of_interest = ['detection_classes', 'detection_boxes', 'detection_scores']\n","        \n","        # filtering out detection dict in order to get only boxes, classes and scores\n","        detections = {key: value for key, value in detections.items() if key in key_of_interest}\n","        \n","        # print('detections processed')\n","        if box_th: # filtering detection if a confidence threshold for boxes was given as a parameter\n","            for key in key_of_interest:\n","                scores = detections['detection_scores']\n","                current_array = detections[key]\n","                filtered_current_array = current_array[scores > box_th]\n","                detections[key] = filtered_current_array\n","        \n","        # print('box_th done')\n","        if nms_th: # filtering rectangles if nms threshold was passed in as a parameter\n","            # creating a zip object that will contain model output info as\n","            output_info = list(zip(detections['detection_boxes'],\n","                                   detections['detection_scores'],\n","                                   detections['detection_classes']\n","                                  )\n","                              )\n","            boxes, scores, classes = nms(output_info)\n","            \n","            detections['detection_boxes'] = boxes # format: [y1, x1, y2, x2]\n","            detections['detection_scores'] = scores\n","            detections['detection_classes'] = classes\n","        \n","        # print('nms_th done')\n","        if to_file and data: # if saving to txt file was requested\n","\n","            image_h, image_w, _ = image_np.shape\n","            file_name = f'pred_result_{data}.txt'\n","            \n","            line2write = list()\n","            line2write.append(os.path.basename(image_path))\n","            \n","            with open(file_name, 'a+') as text_file:\n","                # iterating over boxes\n","                for b, s, c in zip(boxes, scores, classes):\n","                    \n","                    y1abs, x1abs = b[0] * image_h, b[1] * image_w\n","                    y2abs, x2abs = b[2] * image_h, b[3] * image_w\n","                    \n","                    list2append = [x1abs, y1abs, x2abs, y2abs, s, c]\n","                    line2append = ','.join([str(item) for item in list2append])\n","                    \n","                    line2write.append(line2append)\n","                \n","                line2write = ' '.join(line2write)\n","                text_file.write(line2write + os.linesep)\n","        # print('to_file done')\n","        \n","    return times"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["test_images_folder = './data/test_panels/'\n","path2images = []\n","for filename in os.listdir(test_images_folder):\n","    path2images.append(os.path.join(test_images_folder,filename))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# import numpy as np\n","# times = inference_as_raw_output(path2images)\n","# arr = np.array(times)\n"," \n","# # measures of dispersion\n","# min = np.amin(arr)\n","# max = np.amax(arr)\n","# range = np.ptp(arr)\n","# variance = np.var(arr)\n","# sd = np.std(arr)\n"," \n","# # print(\"Array =\", arr)\n","# print(\"Measures of Dispersion\")\n","# print(\"Minimum =\", min)\n","# print(\"Maximum =\", max)\n","# print(\"Range =\", range)\n","# print(\"Variance =\", variance)\n","# print(\"Standard Deviation =\", sd)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["  #Functions for button detection\n","  import cv2\n","  import PIL\n","  import PIL.Image\n","  import io\n","\n","  def button_candidates(boxes, scores, img_path):\n","      \n","    with open(img_path, 'rb') as f:\n","            image = np.asarray(PIL.Image.open(io.BytesIO(f.read())))\n","    img_height = image.shape[0]\n","    img_width = image.shape[1]\n","\n","    button_scores = [] #stores the score of each button (confidence)\n","    button_patches = [] #stores the cropped image that encloses the button\n","    button_positions = [] #stores the coordinates of the bounding box on buttons\n","\n","    for box, score in zip(boxes, scores):\n","      if score < 0.5: continue\n","\n","      y_min = int(box[0] * img_height)\n","      x_min = int(box[1] * img_width)\n","      y_max = int(box[2] * img_height)\n","      x_max = int(box[3] * img_width)\n","\n","      button_patch = image[y_min: y_max, x_min: x_max]\n","      button_patch = cv2.resize(button_patch, (180, 180))\n","\n","      button_scores.append(score)\n","      button_patches.append(button_patch)\n","      button_positions.append([x_min, y_min, x_max, y_max])\n","      \n","    return button_patches, button_positions, button_scores\n","\n","\n","  def inferences_and_times(image_path,box_th = 0.5,nms_th = 0.5,to_file = False,data = None,path2dir = False):\n","      # print(\"Start\")\n","      \n","      \"\"\"\n","      Function that performs inference and return filtered predictions and prediction detections\n","      \n","      Args:\n","        path2images: an array with pathes to images\n","        box_th: (float) value that defines threshold for model prediction. Consider 0.25 as a value.\n","        nms_th: (float) value that defines threshold for non-maximum suppression. Consider 0.5 as a value.\n","        to_file: (boolean). When passed as True => results are saved into a file. Writing format is\n","        path2image + (x1abs, y1abs, x2abs, y2abs, score, conf) for box in boxes\n","        data: (str) name of the dataset you passed in (e.g. test/validation)\n","        path2dir: (str). Should be passed if path2images has only basenames. If full pathes provided => set False.\n","        \n","      Returs:\n","        detections (dict): filtered predictions that model made\n","      \"\"\"\n","      \n","\n","      if path2dir: # if a path to a directory where images are stored was passed in\n","          image_path = os.path.join(path2dir, image_path.strip())\n","          \n","      image_np = load_image_into_numpy_array(image_path)\n","\n","      input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","      detections = detect_fn(input_tensor)\n","      # checking how many detections we got\n","      num_detections = int(detections.pop('num_detections'))\n","      \n","      # filtering out detection in order to get only the one that are indeed detections\n","      detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n","      \n","      # detection_classes should be ints.\n","      detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","      \n","      # defining what we need from the resulting detection dict that we got from model output\n","      key_of_interest = ['detection_classes', 'detection_boxes', 'detection_scores']\n","      \n","      # filtering out detection dict in order to get only boxes, classes and scores\n","      detections = {key: value for key, value in detections.items() if key in key_of_interest}\n","      \n","      # print('detections processed')\n","      if box_th: # filtering detection if a confidence threshold for boxes was given as a parameter\n","          for key in key_of_interest:\n","              scores = detections['detection_scores']\n","              current_array = detections[key]\n","              filtered_current_array = current_array[scores > box_th]\n","              detections[key] = filtered_current_array\n","      \n","      # print('box_th done')\n","      if nms_th: # filtering rectangles if nms threshold was passed in as a parameter\n","          # creating a zip object that will contain model output info as\n","          output_info = list(zip(detections['detection_boxes'],\n","                                  detections['detection_scores'],\n","                                  detections['detection_classes']\n","                                  )\n","                              )\n","          boxes, scores, classes = nms(output_info)\n","          \n","          detections['detection_boxes'] = boxes # format: [y1, x1, y2, x2]\n","          detections['detection_scores'] = scores\n","          detections['detection_classes'] = classes\n","      \n","      # print('nms_th done')\n","      if to_file and data: # if saving to txt file was requested\n","\n","          image_h, image_w, _ = image_np.shape\n","          file_name = f'pred_result_{data}.txt'\n","          \n","          line2write = list()\n","          line2write.append(os.path.basename(image_path))\n","          \n","          with open(file_name, 'a+') as text_file:\n","              # iterating over boxes\n","              for b, s, c in zip(boxes, scores, classes):\n","                  \n","                  y1abs, x1abs = b[0] * image_h, b[1] * image_w\n","                  y2abs, x2abs = b[2] * image_h, b[3] * image_w\n","                  \n","                  list2append = [x1abs, y1abs, x2abs, y2abs, s, c]\n","                  line2append = ','.join([str(item) for item in list2append])\n","                  \n","                  line2write.append(line2append)\n","              \n","              line2write = ' '.join(line2write)\n","              text_file.write(line2write + os.linesep)\n","      # print('to_file done')\n","          \n","      return detections"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import os\n","import imageio\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","from PIL import Image, ImageDraw, ImageFont\n","\n","\n","charset = {'0': 0,  '1': 1,  '2': 2,  '3': 3,  '4': 4,  '5': 5,\n","           '6': 6,  '7': 7,  '8': 8,  '9': 9,  'A': 10, 'B': 11,\n","           'C': 12, 'D': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17,\n","           'I': 18, 'J': 19, 'K': 20, 'L': 21, 'M': 22, 'N': 23,\n","           'O': 24, 'P': 25, 'R': 26, 'S': 27, 'T': 28, 'U': 29,\n","           'V': 30, 'X': 31, 'Z': 32, '<': 33, '>': 34, '(': 35,\n","           ')': 36, '$': 37, '#': 38, '^': 39, 's': 40, '-': 41,\n","           '*': 42, '%': 43, '?': 44, '!': 45, '+': 46} # <nul> = +\n","\n","class CharacterRecognizer:\n","  def __init__(self, graph_path=None, verbose=False):\n","    self.graph_path = graph_path #path to the model which is loaded as a graph\n","    self.session = None\n","    self.input = None\n","    self.output = []\n","    self.class_num = 1\n","    self.verbose = verbose\n","\n","    self.idx_lbl = {} #this is the functionally inverse to charset\n","    for key in charset.keys():\n","      self.idx_lbl[charset[key]] = key\n","    self.init_recognizer()\n","    print('character recognizer initialized!')\n","\n","  def init_recognizer(self):\n","\n","    # load graph and label map from default folder\n","    if self.graph_path is None:\n","      self.graph_path = './frozen_models/ocr_graph.pb'\n","\n","    # check existence of the two files\n","    if not os.path.exists(self.graph_path):\n","      raise IOError('Invalid ocr_graph path! {}'.format(self.graph_path))\n","\n","    # load frozen graph\n","    recognition_graph = tf.Graph()\n","    with recognition_graph.as_default():\n","      od_graph_def = tf.GraphDef()\n","      with tf.gfile.GFile(self.graph_path, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","    self.session = tf.Session(graph=recognition_graph)\n","\n","    # prepare input and output request\n","    self.input = recognition_graph.get_tensor_by_name('ocr_input:0')\n","    # self.output.append(recognition_graph.get_tensor_by_name('chars_logit:0'))\n","    # self.output.append(recognition_graph.get_tensor_by_name('chars_log_prob:0'))\n","    self.output.append(recognition_graph.get_tensor_by_name('predicted_chars:0'))\n","    self.output.append(recognition_graph.get_tensor_by_name('predicted_scores:0'))\n","    # self.output.append(recognition_graph.get_tensor_by_name('predicted_text:0'))\n","\n","\n","  def clear_session(self):\n","    if self.session is not None:\n","      self.session.close()\n","\n","  def predict(self, image_np, draw=False):\n","    assert image_np.shape == (180, 180, 3)\n","    img_in = np.expand_dims(image_np, axis=0)\n","    codes, scores = self.session.run(self.output, feed_dict={self.input: img_in}) #returns codes and scores for each code (single letter)\n","    codes, scores = [np.squeeze(x) for x in [codes, scores]]\n","    print(len(codes), codes)\n","    score_ave = 0\n","    text = ''\n","    for char, score in zip(codes, scores):\n","      if not self.idx_lbl[char] == '+':\n","        score_ave += score\n","        text += self.idx_lbl[char]\n","    score_ave /= len(text)\n","\n","    if self.verbose:\n","      self.visualize_recognition_result(image_np, text, score_ave)\n","\n","\n","    img_show = self.draw_result(image_np, text, score_ave) if draw else image_np\n","    \n","    # print(f\"text = {text}\")\n","\n","    return text, score_ave, np.array(img_show)\n","\n","  @staticmethod\n","  def visualize_recognition_result(image_np, text, scores):\n","    img_pil = Image.fromarray(image_np)\n","    img_show = ImageDraw.Draw(img_pil)\n","    font = ImageFont.truetype('./Arial.ttf', 60)\n","    img_show.text((45, 60), text=text, font=font, fill=(255, 0, 255))\n","    img_pil.show()\n","\n","  @staticmethod\n","  def draw_result(image_np, text, scores):\n","    img_pil = Image.fromarray(image_np)\n","    img_show = ImageDraw.Draw(img_pil)\n","    font = ImageFont.truetype('./Arial.ttf', 60)\n","    img_show.text((45, 60), text=text, font=font, fill=(255, 0, 255))\n","    return img_pil\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["path2images = []\n","test_panels = './data/test_panels/'\n","for filename in os.listdir(test_panels):\n","    file_path = os.path.join(test_panels, filename)\n","    path2images.append(file_path)    "]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["character recognizer initialized!\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Tensor(\"Preprocessor_1/stack:0\", shape=(1, 512, 512, 3), dtype=float32) Tensor(\"Preprocessor_1/stack_1:0\", shape=(1, 3), dtype=int32)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"ValueError","evalue":"Your Layer or Model is in an invalid state. This can happen for the following cases:\n 1. You might be interleaving estimator/non-estimator models or interleaving models/layers made in tf.compat.v1.Graph.as_default() with models/layers created outside of it. Converting a model to an estimator (via model_to_estimator) invalidates all models/layers made before the conversion (even if they were not the model converted to an estimator). Similarly, making a layer or a model inside a a tf.compat.v1.Graph invalidates all layers/models you previously made outside of the graph.\n2. You might be using a custom keras layer implementation with custom __init__ which didn't call super().__init__.  Please check the implementation of <class 'object_detection.models.ssd_efficientnet_bifpn_feature_extractor.SSDEfficientNetB0BiFPNKerasFeatureExtractor'> and its bases.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m overall_lbl_times \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m image_path \u001b[39min\u001b[39;00m tqdm(path2images[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]):\n\u001b[0;32m----> 6\u001b[0m     time_det, dets \u001b[39m=\u001b[39m inferences_and_times(image_path)\n\u001b[1;32m      7\u001b[0m     boxes, scores, classes \u001b[39m=\u001b[39m dets[\u001b[39m'\u001b[39m\u001b[39mdetection_boxes\u001b[39m\u001b[39m'\u001b[39m], dets[\u001b[39m'\u001b[39m\u001b[39mdetection_scores\u001b[39m\u001b[39m'\u001b[39m], dets[\u001b[39m'\u001b[39m\u001b[39mdetection_classes\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     boxes, scores, classes \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39msqueeze(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [boxes, scores, classes]]\n","Cell \u001b[0;32mIn[26], line 62\u001b[0m, in \u001b[0;36minferences_and_times\u001b[0;34m(image_path, box_th, nms_th, to_file, data, path2dir)\u001b[0m\n\u001b[1;32m     59\u001b[0m image_np \u001b[39m=\u001b[39m load_image_into_numpy_array(image_path)\n\u001b[1;32m     61\u001b[0m input_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(np\u001b[39m.\u001b[39mexpand_dims(image_np, \u001b[39m0\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> 62\u001b[0m detections \u001b[39m=\u001b[39m detect_fn(input_tensor)\n\u001b[1;32m     63\u001b[0m \u001b[39m# checking how many detections we got\u001b[39;00m\n\u001b[1;32m     64\u001b[0m num_detections \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(detections\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mnum_detections\u001b[39m\u001b[39m'\u001b[39m))\n","Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36mdetect_fn\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     12\u001b[0m image, shapes \u001b[39m=\u001b[39m detection_model\u001b[39m.\u001b[39mpreprocess(image)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(image,shapes)\n\u001b[0;32m---> 14\u001b[0m prediction_dict \u001b[39m=\u001b[39m detection_model\u001b[39m.\u001b[39;49mpredict(image, shapes)\n\u001b[1;32m     15\u001b[0m detections \u001b[39m=\u001b[39m detection_model\u001b[39m.\u001b[39mpostprocess(prediction_dict, shapes)\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m detections\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/object_detection/meta_architectures/ssd_meta_arch.py:571\u001b[0m, in \u001b[0;36mSSDMetaArch.predict\u001b[0;34m(self, preprocessed_inputs, true_image_shapes)\u001b[0m\n\u001b[1;32m    569\u001b[0m   batchnorm_updates_collections \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39mUPDATE_OPS\n\u001b[1;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_extractor\u001b[39m.\u001b[39mis_keras_model:\n\u001b[0;32m--> 571\u001b[0m   feature_maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feature_extractor(preprocessed_inputs)\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m   \u001b[39mwith\u001b[39;00m slim\u001b[39m.\u001b[39marg_scope([slim\u001b[39m.\u001b[39mbatch_norm],\n\u001b[1;32m    574\u001b[0m                       is_training\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_training \u001b[39mand\u001b[39;00m\n\u001b[1;32m    575\u001b[0m                                    \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_freeze_batchnorm),\n\u001b[1;32m    576\u001b[0m                       updates_collections\u001b[39m=\u001b[39mbatchnorm_updates_collections):\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer_v1.py:906\u001b[0m, in \u001b[0;36mLayer._assert_built_as_v1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_built_as_v1\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_originally_built_as_v1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 906\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    907\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYour Layer or Model is in an invalid state. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    908\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis can happen for the following cases:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m1. You might be interleaving estimator/non-estimator models \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor interleaving models/layers made in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtf.compat.v1.Graph.as_default() with models/layers created \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutside of it. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConverting a model to an estimator (via model_to_estimator) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minvalidates all models/layers made before the conversion \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    915\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(even if they were not the model converted to an estimator). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSimilarly, making a layer or a model inside a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39ma tf.compat.v1.Graph invalidates all layers/models you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    918\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpreviously made outside of the graph.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    919\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m2. You might be using a custom keras layer implementation \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith custom __init__ which didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt call super().__init__. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    921\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m Please check the implementation of \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and its bases.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m             \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m),)\n\u001b[1;32m    923\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Your Layer or Model is in an invalid state. This can happen for the following cases:\n 1. You might be interleaving estimator/non-estimator models or interleaving models/layers made in tf.compat.v1.Graph.as_default() with models/layers created outside of it. Converting a model to an estimator (via model_to_estimator) invalidates all models/layers made before the conversion (even if they were not the model converted to an estimator). Similarly, making a layer or a model inside a a tf.compat.v1.Graph invalidates all layers/models you previously made outside of the graph.\n2. You might be using a custom keras layer implementation with custom __init__ which didn't call super().__init__.  Please check the implementation of <class 'object_detection.models.ssd_efficientnet_bifpn_feature_extractor.SSDEfficientNetB0BiFPNKerasFeatureExtractor'> and its bases."]}],"source":["recognizer = CharacterRecognizer(verbose=False)\n","overall_det_times = []\n","overall_lbl_times = []\n","\n","for image_path in tqdm(path2images[0:1]):\n","    time_det, dets = inferences_and_times(image_path)\n","    boxes, scores, classes = dets['detection_boxes'], dets['detection_scores'], dets['detection_classes']\n","    boxes, scores, classes = [np.squeeze(x) for x in [boxes, scores, classes]]\n","    print(boxes.shape, boxes)\n","    print(scores.shape, scores)\n","    print(classes.shape, classes)\n","    button_patches, button_positions, _ = button_candidates(boxes, scores, image_path)\n","    t0 = cv2.getTickCount()\n","    for button_imgs in button_patches:\n","        button_text, button_score, _ = recognizer.predict(button_imgs)\n","    t1 = cv2.getTickCount()\n","    time_lbl = (t1-t0)/cv2.getTickFrequency()\n","    \n","    overall_det_times.append(time_det)\n","    overall_lbl_times.append(time_lbl)\n","\n","    print(f\"Det time = {time_det}\")\n","    print(f\"Lbl time = {time_lbl}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Current data set is None\n","Ready to start inference on 1 images!\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]\n"]},{"ename":"ValueError","evalue":"Your Layer or Model is in an invalid state. This can happen for the following cases:\n 1. You might be interleaving estimator/non-estimator models or interleaving models/layers made in tf.compat.v1.Graph.as_default() with models/layers created outside of it. Converting a model to an estimator (via model_to_estimator) invalidates all models/layers made before the conversion (even if they were not the model converted to an estimator). Similarly, making a layer or a model inside a a tf.compat.v1.Graph invalidates all layers/models you previously made outside of the graph.\n2. You might be using a custom keras layer implementation with custom __init__ which didn't call super().__init__.  Please check the implementation of <class 'object_detection.models.ssd_efficientnet_bifpn_feature_extractor.SSDEfficientNetB0BiFPNKerasFeatureExtractor'> and its bases.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inference_as_raw_output(path2images[\u001b[39m0\u001b[39;49m:\u001b[39m1\u001b[39;49m])\n","Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36minference_as_raw_output\u001b[0;34m(path2images, box_th, nms_th, to_file, data, path2dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m input_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(np\u001b[39m.\u001b[39mexpand_dims(image_np, \u001b[39m0\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     33\u001b[0m t0 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mgetTickCount()\n\u001b[0;32m---> 34\u001b[0m detections \u001b[39m=\u001b[39m detect_fn(input_tensor)\n\u001b[1;32m     35\u001b[0m t1 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mgetTickCount()\n\u001b[1;32m     36\u001b[0m time \u001b[39m=\u001b[39m (t1\u001b[39m-\u001b[39mt0)\u001b[39m/\u001b[39mcv2\u001b[39m.\u001b[39mgetTickFrequency()\n","Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mdetect_fn\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mDetect objects in image.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m  detections (dict): predictions that model made\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m image, shapes \u001b[39m=\u001b[39m detection_model\u001b[39m.\u001b[39mpreprocess(image)\n\u001b[0;32m---> 13\u001b[0m prediction_dict \u001b[39m=\u001b[39m detection_model\u001b[39m.\u001b[39;49mpredict(image, shapes)\n\u001b[1;32m     14\u001b[0m detections \u001b[39m=\u001b[39m detection_model\u001b[39m.\u001b[39mpostprocess(prediction_dict, shapes)\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m detections\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/object_detection/meta_architectures/ssd_meta_arch.py:571\u001b[0m, in \u001b[0;36mSSDMetaArch.predict\u001b[0;34m(self, preprocessed_inputs, true_image_shapes)\u001b[0m\n\u001b[1;32m    569\u001b[0m   batchnorm_updates_collections \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39mUPDATE_OPS\n\u001b[1;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_extractor\u001b[39m.\u001b[39mis_keras_model:\n\u001b[0;32m--> 571\u001b[0m   feature_maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feature_extractor(preprocessed_inputs)\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m   \u001b[39mwith\u001b[39;00m slim\u001b[39m.\u001b[39marg_scope([slim\u001b[39m.\u001b[39mbatch_norm],\n\u001b[1;32m    574\u001b[0m                       is_training\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_training \u001b[39mand\u001b[39;00m\n\u001b[1;32m    575\u001b[0m                                    \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_freeze_batchnorm),\n\u001b[1;32m    576\u001b[0m                       updates_collections\u001b[39m=\u001b[39mbatchnorm_updates_collections):\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer_v1.py:906\u001b[0m, in \u001b[0;36mLayer._assert_built_as_v1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_built_as_v1\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_originally_built_as_v1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 906\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    907\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYour Layer or Model is in an invalid state. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    908\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis can happen for the following cases:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m1. You might be interleaving estimator/non-estimator models \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor interleaving models/layers made in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtf.compat.v1.Graph.as_default() with models/layers created \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutside of it. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConverting a model to an estimator (via model_to_estimator) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minvalidates all models/layers made before the conversion \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    915\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(even if they were not the model converted to an estimator). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSimilarly, making a layer or a model inside a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39ma tf.compat.v1.Graph invalidates all layers/models you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    918\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpreviously made outside of the graph.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    919\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m2. You might be using a custom keras layer implementation \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith custom __init__ which didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt call super().__init__. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    921\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m Please check the implementation of \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and its bases.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m             \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m),)\n\u001b[1;32m    923\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Your Layer or Model is in an invalid state. This can happen for the following cases:\n 1. You might be interleaving estimator/non-estimator models or interleaving models/layers made in tf.compat.v1.Graph.as_default() with models/layers created outside of it. Converting a model to an estimator (via model_to_estimator) invalidates all models/layers made before the conversion (even if they were not the model converted to an estimator). Similarly, making a layer or a model inside a a tf.compat.v1.Graph invalidates all layers/models you previously made outside of the graph.\n2. You might be using a custom keras layer implementation with custom __init__ which didn't call super().__init__.  Please check the implementation of <class 'object_detection.models.ssd_efficientnet_bifpn_feature_extractor.SSDEfficientNetB0BiFPNKerasFeatureExtractor'> and its bases."]}],"source":["inference_as_raw_output(path2images[0:1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["python3 model_main_tf2.py --pipeline_config_path=./models/efficientdet_do/v3/pipeline.config --model_dir=./models/efficientdet_do/v3/ --checkpoint_every_n=500 --num_workers=4 --alsologtostderr"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["python3 model_main_tf2.py --pipeline_config_path=./models/efficientdet_do/v3/pipeline.config --model_dir=./models/efficientdet_do/v3/ --checkpoint_dir=./models/efficientdet_do/v3/ --num_workers=4 --sample_1_of_n_eval_examples=1"]}],"metadata":{"kernelspec":{"display_name":"eff_det","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"neptune":{"notebookId":"7c618cd5-39ec-46c6-bee7-0cfe5297f22a"}},"nbformat":4,"nbformat_minor":4}
